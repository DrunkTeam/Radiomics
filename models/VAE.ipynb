{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##VAE"
      ],
      "metadata": {
        "id": "70XAiGK71WYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "HD4IzDll2S79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file = '/content/without.zip'\n",
        "\n",
        "rootdir=zipfile.ZipFile(zip_file, 'r')\n",
        "rootdir.extractall()"
      ],
      "metadata": {
        "id": "1gVN1NMX21N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, folder_path, transform=None):\n",
        "        self.folder_path = folder_path\n",
        "        self.transform = transform\n",
        "        self.image_files = os.listdir(folder_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        image_name = os.path.join(self.folder_path, self.image_files[idx])\n",
        "        image = Image.open(image_name)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "# Путь к папке с изображениями\n",
        "folder_path = \"/content/archive\"\n",
        "\n",
        "# Преобразования для предварительной обработки изображений\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Создание экземпляра Dataset и DataLoader\n",
        "custom_dataset = CustomDataset(folder_path, transform=transform)\n",
        "\n",
        "# Разделение данных на тренировочную и тестовую выборки\n",
        "train_data, test_data = train_test_split(custom_dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "batchsize=64\n",
        "\n",
        "# Создание DataLoader для тренировочной и тестовой выборок\n",
        "trainloader = DataLoader(train_data, batch_size=batchsize, shuffle=True)\n",
        "testloader = DataLoader(test_data, batch_size=batchsize, shuffle=False)\n",
        "\n",
        "dataloader = DataLoader(custom_dataset, batch_size=batchsize, shuffle=True)"
      ],
      "metadata": {
        "id": "EB6UFc5R7jXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_A3By6ev1GEq",
        "outputId": "bb821ca2-3e28-4b5f-ff5c-cf330d4e6dc6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VAE(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu1): ReLU(inplace=True)\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu2): ReLU(inplace=True)\n",
              "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu3): ReLU(inplace=True)\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (mu): Linear(in_features=131072, out_features=40, bias=True)\n",
              "  (std): Linear(in_features=131072, out_features=40, bias=True)\n",
              "  (fc): Linear(in_features=40, out_features=131072, bias=True)\n",
              "  (deconv1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "  (dec_bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (dec_relu1): ReLU(inplace=True)\n",
              "  (deconv2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "  (dec_bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (dec_relu2): ReLU(inplace=True)\n",
              "  (deconv3): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "## Template code\n",
        "## Write your code here\n",
        "# Convolution formula: ((n + 2p - f) / s) + 1\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "        #Encoder part\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1) #256*256*1 -> 128*128*32\n",
        "        self.bn1=nn.BatchNorm2d(32)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1) #128*128*32 -> 64*64*64\n",
        "        self.bn2=nn.BatchNorm2d(64)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1) #64*64*64  -> 32*32*128\n",
        "        self.bn3=nn.BatchNorm2d(128)\n",
        "        self.relu3 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.mu = nn.Linear(32*32*128, 40)  # 32*32*128\n",
        "        self.std = nn.Linear(32*32*128, 40)  # 32*32*128\n",
        "\n",
        "        # Decoder part\n",
        "        self.fc = nn.Linear(40, 32*32*128)\n",
        "        self.deconv1 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.dec_bn1 = nn.BatchNorm2d(64)\n",
        "        self.dec_relu1 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.deconv2 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
        "        self.dec_bn2 = nn.BatchNorm2d(32)\n",
        "        self.dec_relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.deconv3 = nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "      std = torch.exp(log_var/2)#>=0\n",
        "      eps = torch.randn_like(std)\n",
        "      return mu + eps * std\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      #Encoder part\n",
        "      x = self.relu1(self.bn1(self.conv1(x)))\n",
        "      x = self.relu2(self.bn2(self.conv2(x)))\n",
        "      x = self.relu3(self.bn3(self.conv3(x)))\n",
        "      x = self.flatten(x)\n",
        "      mu = self.mu(x)\n",
        "      logvar = self.std(x)\n",
        "\n",
        "      z = self.reparameterize(mu, logvar)\n",
        "\n",
        "      #Decoder part\n",
        "      x = self.fc(z)\n",
        "      x = x.view(x.size(0), 128, 32, 32)\n",
        "      x = self.dec_relu1(self.dec_bn1(self.deconv1(x)))\n",
        "      x = self.dec_relu2(self.dec_bn2(self.deconv2(x)))\n",
        "      recon_x = self.sigmoid(self.deconv3(x))\n",
        "      return recon_x, mu, logvar\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "      x = self.relu1(self.bn1(self.conv1(x)))\n",
        "      x = self.relu2(self.bn2(self.conv2(x)))\n",
        "      x = self.relu3(self.bn3(self.conv3(x)))\n",
        "      x = self.flatten(x)\n",
        "\n",
        "      mu = self.fc_mu(x)\n",
        "      logvar = self.fc_logvar(x)\n",
        "      return mu, logvar\n",
        "\n",
        "    def decode(self, z):\n",
        "      x = self.fc(z)\n",
        "      x = x.view(x.size(0), 128, 32, 32)\n",
        "      x = self.dec_relu1(self.dec_bn1(self.deconv1(x)))\n",
        "      x = self.dec_relu2(self.dec_bn2(self.deconv2(x)))\n",
        "      recon_x = self.sigmoid(self.deconv3(x))\n",
        "      return recon_x\n",
        "\n",
        "vae_model = VAE()\n",
        "vae_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate=0.001\n",
        "optimizer = torch.optim.Adam(vae_model.parameters(), lr=learning_rate)\n",
        "num_epochs = 100\n",
        "\n",
        "mse_loss = nn.MSELoss(reduction='sum')\n",
        "\n",
        "# Start training\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    total_loss=0\n",
        "    for i, x in enumerate(trainloader):\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        x_reconst, mu, log_var = vae_model(x)\n",
        "\n",
        "        # Compute reconstruction loss and kl divergence\n",
        "        reconst_loss = mse_loss(x_reconst, x)\n",
        "        kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "\n",
        "        # Backprop and optimize\n",
        "        loss = reconst_loss + kl_div\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "          #посмотреть trainset, его нет в Dataloader, который я наджипитила, надо переделать dataloader\n",
        "            print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, Loss: {:.4f}\"\n",
        "                   .format(epoch+1, num_epochs, i+1, len(trainloader), reconst_loss.item(), total_loss / len(trainset)))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Save the sampled images\n",
        "        z = torch.randn(batchsize, 40).to(device)\n",
        "        out = vae_model.decode(z).view(-1, 1, 32, 32)\n",
        "        save_image(out,'./sampled-{}.png'.format(epoch+1))\n",
        "\n",
        "        # Save the reconstructed images\n",
        "        out, _, _ = vae_model(x)\n",
        "        # print('out', out.shape, '\\n')\n",
        "        # print('x', x.shape)\n",
        "        x_concat = torch.cat([x.view(-1, 1, 32, 32), out.view(-1, 1, 32, 32)], dim=1)\n",
        "        save_image(x_concat, './reconst-{}.png'.format(epoch+1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_SDCtdp1dPV",
        "outputId": "9f360eba-8ff8-4e4a-c5f4-b2c45f2979ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [56:06<00:00, 33.66s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bjDOm8rH1hUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a function that will generate images using the implemented vae"
      ],
      "metadata": {
        "id": "-oC1yClp1pbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def generate_images(vae, num_images=5):\n",
        "    vae.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        # Generate random latent vectors\n",
        "        latent_vectors = torch.randn(num_images, 40)\n",
        "\n",
        "        # Decode latent vectors to generate images\n",
        "        generated_images = vae.decode(latent_vectors)\n",
        "\n",
        "    # Plot the generated images\n",
        "    plt.figure(figsize=(10, 2))\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "        image = generated_images[i].permute(1, 2, 0).numpy()\n",
        "        plt.imshow(np.clip(image, 0, 1))\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Assuming 'vae' is the trained VAE model\n",
        "generate_images(vae_model, num_images=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "duM_Ey321hyl",
        "outputId": "a96e112e-4fe2-4af5-b427-5027a4f020da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x200 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0mleNr2m1h35"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}